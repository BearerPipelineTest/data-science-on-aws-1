{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf5dc9c-5f7c-45dd-a702-89fb0ecf58e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 07:59:24,600\tINFO trainer.py:243 -- Trainer logs will be logged in: /home/ray/ray_results/train_2022-06-24_07-59-24\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m 2022-06-24 07:59:29,071\tINFO torch.py:347 -- Setting up process group for: env:// [rank=1, world_size=2]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2502)\u001b[0m 2022-06-24 07:59:29,087\tINFO torch.py:347 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "2022-06-24 07:59:30,109\tINFO trainer.py:249 -- Run results will be logged in: /home/ray/ray_results/train_2022-06-24_07-59-24/run_001\n",
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 11023.14it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1619.11it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 908.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m Downloading and preparing dataset csv/default to /home/ray/.cache/huggingface/datasets/csv/default-3a58b73918809981/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m Dataset csv downloaded and prepared to /home/ray/.cache/huggingface/datasets/csv/default-3a58b73918809981/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 810.89it/s]\n",
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 438kB/s]\n",
      "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]\n",
      "Downloading:   7%|▋         | 64.0k/878k [00:00<00:01, 505kB/s]\n",
      "Downloading:  15%|█▍        | 128k/878k [00:00<00:01, 503kB/s] \n",
      "Downloading:  22%|██▏       | 192k/878k [00:00<00:01, 503kB/s]\n",
      "Downloading: 100%|██████████| 878k/878k [00:00<00:00, 1.70MB/s]\n",
      "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]\n",
      "Downloading:  16%|█▌        | 70.0k/446k [00:00<00:00, 506kB/s]\n",
      "Downloading:  30%|███       | 134k/446k [00:00<00:00, 479kB/s] \n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 1.07MB/s]\n",
      "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]\n",
      "Downloading:   5%|▌         | 72.0k/1.29M [00:00<00:02, 559kB/s]\n",
      "Downloading:  15%|█▌        | 200k/1.29M [00:00<00:01, 812kB/s] \n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 3.33MB/s]\n",
      "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]\n",
      "Downloading:   0%|          | 891k/478M [00:00<00:55, 9.04MB/s]\n",
      "Downloading:   1%|          | 5.40M/478M [00:00<00:15, 31.6MB/s]\n",
      "Downloading:   3%|▎         | 12.3M/478M [00:00<00:09, 50.3MB/s]\n",
      "Downloading:   4%|▍         | 19.3M/478M [00:00<00:08, 59.1MB/s]\n",
      "Downloading:   5%|▌         | 26.1M/478M [00:00<00:07, 63.6MB/s]\n",
      "Downloading:   7%|▋         | 33.1M/478M [00:00<00:06, 66.9MB/s]\n",
      "Downloading:   8%|▊         | 40.1M/478M [00:00<00:06, 69.1MB/s]\n",
      "Downloading:  10%|▉         | 47.1M/478M [00:00<00:06, 70.4MB/s]\n",
      "Downloading:  11%|█▏        | 54.1M/478M [00:00<00:06, 71.3MB/s]\n",
      "Downloading:  13%|█▎        | 61.1M/478M [00:01<00:06, 72.0MB/s]\n",
      "Downloading:  14%|█▍        | 68.0M/478M [00:01<00:05, 72.1MB/s]\n",
      "Downloading:  16%|█▌        | 75.0M/478M [00:01<00:05, 72.5MB/s]\n",
      "Downloading:  17%|█▋        | 82.0M/478M [00:01<00:05, 72.7MB/s]\n",
      "Downloading:  19%|█▊        | 89.0M/478M [00:01<00:05, 73.0MB/s]\n",
      "Downloading:  20%|██        | 96.0M/478M [00:01<00:05, 73.1MB/s]\n",
      "Downloading:  22%|██▏       | 103M/478M [00:01<00:05, 73.3MB/s] \n",
      "Downloading:  23%|██▎       | 110M/478M [00:01<00:05, 73.3MB/s]\n",
      "Downloading:  24%|██▍       | 117M/478M [00:01<00:05, 73.3MB/s]\n",
      "Downloading:  26%|██▌       | 124M/478M [00:01<00:05, 73.3MB/s]\n",
      "Downloading:  27%|██▋       | 131M/478M [00:02<00:04, 73.3MB/s]\n",
      "Downloading:  29%|██▉       | 138M/478M [00:02<00:04, 73.2MB/s]\n",
      "Downloading:  30%|███       | 145M/478M [00:02<00:04, 73.2MB/s]\n",
      "Downloading:  32%|███▏      | 152M/478M [00:02<00:04, 73.2MB/s]\n",
      "Downloading:  33%|███▎      | 159M/478M [00:02<00:04, 73.3MB/s]\n",
      "Downloading:  35%|███▍      | 166M/478M [00:02<00:04, 73.4MB/s]\n",
      "Downloading:  36%|███▌      | 173M/478M [00:02<00:04, 73.4MB/s]\n",
      "Downloading:  38%|███▊      | 180M/478M [00:02<00:04, 73.5MB/s]\n",
      "Downloading:  39%|███▉      | 187M/478M [00:02<00:04, 73.5MB/s]\n",
      "Downloading:  41%|████      | 194M/478M [00:02<00:04, 73.4MB/s]\n",
      "Downloading:  42%|████▏     | 201M/478M [00:03<00:03, 73.5MB/s]\n",
      "Downloading:  44%|████▎     | 208M/478M [00:03<00:03, 73.5MB/s]\n",
      "Downloading:  45%|████▌     | 215M/478M [00:03<00:03, 73.5MB/s]\n",
      "Downloading:  46%|████▋     | 222M/478M [00:03<00:03, 73.4MB/s]\n",
      "Downloading:  48%|████▊     | 229M/478M [00:03<00:03, 73.4MB/s]\n",
      "Downloading:  49%|████▉     | 236M/478M [00:03<00:03, 73.5MB/s]\n",
      "Downloading:  51%|█████     | 243M/478M [00:03<00:03, 73.5MB/s]\n",
      "Downloading:  52%|█████▏    | 250M/478M [00:03<00:03, 73.5MB/s]\n",
      "Downloading:  54%|█████▍    | 257M/478M [00:03<00:03, 73.5MB/s]\n",
      "Downloading:  57%|█████▋    | 271M/478M [00:04<00:02, 73.6MB/s]\n",
      "Downloading:  58%|█████▊    | 278M/478M [00:04<00:02, 73.6MB/s]\n",
      "Downloading:  60%|█████▉    | 285M/478M [00:04<00:02, 73.6MB/s]\n",
      "Downloading:  61%|██████    | 292M/478M [00:04<00:02, 73.6MB/s]\n",
      "Downloading:  63%|██████▎   | 299M/478M [00:04<00:02, 73.8MB/s]\n",
      "Downloading:  64%|██████▍   | 306M/478M [00:04<00:02, 73.5MB/s]\n",
      "Downloading:  66%|██████▌   | 313M/478M [00:04<00:02, 73.5MB/s]\n",
      "Downloading:  67%|██████▋   | 321M/478M [00:04<00:02, 73.6MB/s]\n",
      "Downloading:  69%|██████▊   | 328M/478M [00:04<00:02, 73.7MB/s]\n",
      "Downloading:  70%|██████▉   | 335M/478M [00:04<00:02, 73.7MB/s]\n",
      "Downloading:  71%|███████▏  | 342M/478M [00:05<00:01, 73.7MB/s]\n",
      "Downloading:  73%|███████▎  | 349M/478M [00:05<00:01, 73.6MB/s]\n",
      "Downloading:  74%|███████▍  | 356M/478M [00:05<00:01, 73.6MB/s]\n",
      "Downloading:  76%|███████▌  | 363M/478M [00:05<00:01, 73.7MB/s]\n",
      "Downloading:  77%|███████▋  | 370M/478M [00:05<00:01, 73.7MB/s]\n",
      "Downloading:  79%|███████▉  | 377M/478M [00:05<00:01, 73.6MB/s]\n",
      "Downloading:  80%|████████  | 384M/478M [00:05<00:01, 73.6MB/s]\n",
      "Downloading:  82%|████████▏ | 391M/478M [00:05<00:01, 73.6MB/s]\n",
      "Downloading:  83%|████████▎ | 398M/478M [00:05<00:01, 73.5MB/s]\n",
      "Downloading:  85%|████████▍ | 405M/478M [00:05<00:01, 73.5MB/s]\n",
      "Downloading:  86%|████████▌ | 412M/478M [00:06<00:00, 73.7MB/s]\n",
      "Downloading:  88%|████████▊ | 419M/478M [00:06<00:00, 73.7MB/s]\n",
      "Downloading:  89%|████████▉ | 426M/478M [00:06<00:00, 73.7MB/s]\n",
      "Downloading:  91%|█████████ | 433M/478M [00:06<00:00, 73.8MB/s]\n",
      "Downloading:  92%|█████████▏| 440M/478M [00:06<00:00, 73.7MB/s]\n",
      "Downloading:  94%|█████████▎| 447M/478M [00:06<00:00, 73.7MB/s]\n",
      "Downloading:  95%|█████████▌| 454M/478M [00:06<00:00, 73.7MB/s]\n",
      "Downloading:  96%|█████████▋| 461M/478M [00:06<00:00, 73.8MB/s]\n",
      "Downloading:  98%|█████████▊| 468M/478M [00:06<00:00, 73.8MB/s]\n",
      "Downloading: 100%|██████████| 478M/478M [00:06<00:00, 72.2MB/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset:  17%|█▋        | 1/6 [00:00<00:00,  8.69ba/s]\n",
      "Running tokenizer on dataset:  33%|███▎      | 2/6 [00:00<00:00, 13.03ba/s]\n",
      "Running tokenizer on dataset:  50%|█████     | 3/6 [00:00<00:00, 12.77ba/s]\n",
      "Running tokenizer on dataset:  67%|██████▋   | 4/6 [00:00<00:00, 14.23ba/s]\n",
      "Running tokenizer on dataset:  83%|████████▎ | 5/6 [00:00<00:00, 14.56ba/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 6/6 [00:00<00:00, 15.73ba/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 44.62ba/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 6/6 [00:00<00:00, 15.14ba/s]\n",
      "Downloading builder script: 3.19kB [00:00, 2.67MB/s]                   \n",
      "  0%|          | 0/100 [00:00<?, ?it/s] \n",
      "  0%|          | 0/100 [00:00<?, ?it/s] \n",
      "  1%|          | 1/100 [00:04<06:39,  4.04s/it]\n",
      "  1%|          | 1/100 [00:04<06:41,  4.06s/it]\n",
      "  2%|▏         | 2/100 [00:07<06:02,  3.70s/it]\n",
      "  2%|▏         | 2/100 [00:07<06:03,  3.71s/it]\n",
      "  3%|▎         | 3/100 [00:10<05:42,  3.53s/it]\n",
      "  3%|▎         | 3/100 [00:10<05:42,  3.54s/it]\n",
      "  4%|▍         | 4/100 [00:14<05:34,  3.49s/it]\n",
      "  4%|▍         | 4/100 [00:14<05:34,  3.48s/it]\n",
      "  5%|▌         | 5/100 [00:17<05:26,  3.44s/it]\n",
      "  5%|▌         | 5/100 [00:17<05:26,  3.43s/it]\n",
      "  6%|▌         | 6/100 [00:21<05:22,  3.43s/it]\n",
      "  6%|▌         | 6/100 [00:20<05:21,  3.42s/it]\n",
      "  7%|▋         | 7/100 [00:24<05:16,  3.40s/it]\n",
      "  7%|▋         | 7/100 [00:24<05:15,  3.39s/it]\n",
      "  8%|▊         | 8/100 [00:27<05:09,  3.37s/it]\n",
      "  8%|▊         | 8/100 [00:27<05:11,  3.39s/it]\n",
      "  9%|▉         | 9/100 [00:30<05:05,  3.35s/it]\n",
      "  9%|▉         | 9/100 [00:31<05:06,  3.37s/it]\n",
      " 10%|█         | 10/100 [00:34<05:01,  3.35s/it]\n",
      " 10%|█         | 10/100 [00:34<05:02,  3.36s/it]\n",
      " 11%|█         | 11/100 [00:37<04:57,  3.35s/it]\n",
      " 11%|█         | 11/100 [00:37<04:59,  3.37s/it]\n",
      " 12%|█▏        | 12/100 [00:40<04:54,  3.35s/it]\n",
      " 12%|█▏        | 12/100 [00:41<04:54,  3.35s/it]\n",
      " 13%|█▎        | 13/100 [00:44<04:52,  3.36s/it]\n",
      " 13%|█▎        | 13/100 [00:44<04:51,  3.35s/it]\n",
      " 14%|█▍        | 14/100 [00:47<04:47,  3.34s/it]\n",
      " 14%|█▍        | 14/100 [00:47<04:48,  3.36s/it]\n",
      " 15%|█▌        | 15/100 [00:51<04:45,  3.36s/it]\n",
      " 15%|█▌        | 15/100 [00:51<04:44,  3.34s/it]\n",
      " 16%|█▌        | 16/100 [00:54<04:41,  3.35s/it]\n",
      " 16%|█▌        | 16/100 [00:54<04:40,  3.34s/it]\n",
      " 17%|█▋        | 17/100 [00:57<04:37,  3.34s/it]\n",
      " 17%|█▋        | 17/100 [00:57<04:37,  3.35s/it]\n",
      " 18%|█▊        | 18/100 [01:01<04:34,  3.34s/it]\n",
      " 18%|█▊        | 18/100 [01:01<04:33,  3.34s/it]\n",
      " 19%|█▉        | 19/100 [01:04<04:30,  3.34s/it]\n",
      " 19%|█▉        | 19/100 [01:04<04:31,  3.35s/it]\n",
      " 20%|██        | 20/100 [01:07<04:26,  3.34s/it]\n",
      " 20%|██        | 20/100 [01:07<04:26,  3.34s/it]\n",
      " 21%|██        | 21/100 [01:11<04:23,  3.33s/it]\n",
      " 21%|██        | 21/100 [01:11<04:24,  3.35s/it]\n",
      " 22%|██▏       | 22/100 [01:14<04:19,  3.33s/it]\n",
      " 22%|██▏       | 22/100 [01:14<04:20,  3.34s/it]\n",
      " 23%|██▎       | 23/100 [01:17<04:16,  3.33s/it]\n",
      " 23%|██▎       | 23/100 [01:17<04:17,  3.34s/it]\n",
      " 24%|██▍       | 24/100 [01:21<04:13,  3.33s/it]\n",
      " 24%|██▍       | 24/100 [01:21<04:13,  3.33s/it]\n",
      " 25%|██▌       | 25/100 [01:24<04:10,  3.34s/it]\n",
      " 25%|██▌       | 25/100 [01:24<04:10,  3.34s/it]\n",
      " 26%|██▌       | 26/100 [01:27<04:06,  3.33s/it]\n",
      " 26%|██▌       | 26/100 [01:27<04:06,  3.33s/it]\n",
      " 27%|██▋       | 27/100 [01:31<04:03,  3.34s/it]\n",
      " 27%|██▋       | 27/100 [01:31<04:02,  3.33s/it]\n",
      " 28%|██▊       | 28/100 [01:34<03:58,  3.32s/it]\n",
      " 28%|██▊       | 28/100 [01:34<04:00,  3.34s/it]\n",
      " 29%|██▉       | 29/100 [01:37<03:55,  3.32s/it]\n",
      " 29%|██▉       | 29/100 [01:37<03:56,  3.34s/it]\n",
      " 30%|███       | 30/100 [01:41<03:52,  3.32s/it]\n",
      " 30%|███       | 30/100 [01:41<03:53,  3.33s/it]\n",
      " 31%|███       | 31/100 [01:44<03:48,  3.32s/it]\n",
      " 31%|███       | 31/100 [01:44<03:49,  3.32s/it]\n",
      " 32%|███▏      | 32/100 [01:47<03:46,  3.32s/it]\n",
      " 32%|███▏      | 32/100 [01:47<03:45,  3.31s/it]\n",
      " 33%|███▎      | 33/100 [01:50<03:41,  3.31s/it]\n",
      " 33%|███▎      | 33/100 [01:51<03:43,  3.33s/it]\n",
      " 34%|███▍      | 34/100 [01:54<03:38,  3.31s/it]\n",
      " 34%|███▍      | 34/100 [01:54<03:40,  3.34s/it]\n",
      " 35%|███▌      | 35/100 [01:57<03:34,  3.30s/it]\n",
      " 35%|███▌      | 35/100 [01:57<03:36,  3.33s/it]\n",
      " 36%|███▌      | 36/100 [02:00<03:31,  3.30s/it]\n",
      " 36%|███▌      | 36/100 [02:01<03:32,  3.32s/it]\n",
      " 37%|███▋      | 37/100 [02:04<03:27,  3.30s/it]\n",
      " 37%|███▋      | 37/100 [02:04<03:29,  3.33s/it]\n",
      " 38%|███▊      | 38/100 [02:07<03:24,  3.30s/it]\n",
      " 38%|███▊      | 38/100 [02:07<03:26,  3.32s/it]\n",
      " 39%|███▉      | 39/100 [02:10<03:21,  3.30s/it]\n",
      " 39%|███▉      | 39/100 [02:11<03:23,  3.33s/it]\n",
      " 40%|████      | 40/100 [02:14<03:17,  3.30s/it]\n",
      " 40%|████      | 40/100 [02:14<03:19,  3.33s/it]\n",
      " 41%|████      | 41/100 [02:17<03:14,  3.30s/it]\n",
      " 41%|████      | 41/100 [02:17<03:16,  3.33s/it]\n",
      " 42%|████▏     | 42/100 [02:20<03:11,  3.30s/it]\n",
      " 42%|████▏     | 42/100 [02:21<03:12,  3.33s/it]\n",
      " 43%|████▎     | 43/100 [02:23<03:07,  3.30s/it]\n",
      " 43%|████▎     | 43/100 [02:24<03:09,  3.32s/it]\n",
      " 44%|████▍     | 44/100 [02:27<03:04,  3.30s/it]\n",
      " 44%|████▍     | 44/100 [02:27<03:05,  3.32s/it]\n",
      " 45%|████▌     | 45/100 [02:30<03:01,  3.30s/it]\n",
      " 45%|████▌     | 45/100 [02:30<03:02,  3.32s/it]\n",
      " 46%|████▌     | 46/100 [02:33<02:58,  3.30s/it]\n",
      " 46%|████▌     | 46/100 [02:34<02:59,  3.32s/it]\n",
      " 47%|████▋     | 47/100 [02:37<02:54,  3.29s/it]\n",
      " 47%|████▋     | 47/100 [02:37<02:55,  3.32s/it]\n",
      " 48%|████▊     | 48/100 [02:40<02:51,  3.30s/it]\n",
      " 48%|████▊     | 48/100 [02:40<02:52,  3.31s/it]\n",
      " 49%|████▉     | 49/100 [02:43<02:48,  3.31s/it]\n",
      " 49%|████▉     | 49/100 [02:44<02:48,  3.31s/it]\n",
      " 50%|█████     | 50/100 [02:47<02:45,  3.31s/it]\n",
      " 50%|█████     | 50/100 [02:47<02:47,  3.35s/it]\n",
      " 51%|█████     | 51/100 [02:50<02:44,  3.36s/it]\n",
      " 51%|█████     | 51/100 [02:50<02:43,  3.33s/it]\n",
      " 52%|█████▏    | 52/100 [02:53<02:40,  3.33s/it]\n",
      " 52%|█████▏    | 52/100 [02:54<02:39,  3.32s/it]\n",
      " 53%|█████▎    | 53/100 [02:57<02:35,  3.32s/it]\n",
      " 53%|█████▎    | 53/100 [02:57<02:35,  3.31s/it]\n",
      " 54%|█████▍    | 54/100 [03:00<02:32,  3.31s/it]\n",
      " 54%|█████▍    | 54/100 [03:00<02:31,  3.30s/it]\n",
      " 55%|█████▌    | 55/100 [03:03<02:28,  3.30s/it]\n",
      " 55%|█████▌    | 55/100 [03:04<02:28,  3.30s/it]\n",
      " 56%|█████▌    | 56/100 [03:06<02:25,  3.30s/it]\n",
      " 56%|█████▌    | 56/100 [03:07<02:24,  3.30s/it]\n",
      " 57%|█████▋    | 57/100 [03:10<02:21,  3.29s/it]\n",
      " 57%|█████▋    | 57/100 [03:10<02:21,  3.29s/it]\n",
      " 58%|█████▊    | 58/100 [03:13<02:18,  3.29s/it]\n",
      " 58%|█████▊    | 58/100 [03:13<02:18,  3.29s/it]\n",
      " 59%|█████▉    | 59/100 [03:16<02:14,  3.28s/it]\n",
      " 59%|█████▉    | 59/100 [03:17<02:14,  3.28s/it]\n",
      " 60%|██████    | 60/100 [03:20<02:11,  3.29s/it]\n",
      " 60%|██████    | 60/100 [03:20<02:11,  3.29s/it]\n",
      " 61%|██████    | 61/100 [03:23<02:08,  3.29s/it]\n",
      " 61%|██████    | 61/100 [03:23<02:08,  3.29s/it]\n",
      " 62%|██████▏   | 62/100 [03:26<02:05,  3.29s/it]\n",
      " 62%|██████▏   | 62/100 [03:27<02:05,  3.29s/it]\n",
      " 63%|██████▎   | 63/100 [03:29<02:01,  3.30s/it]\n",
      " 63%|██████▎   | 63/100 [03:30<02:01,  3.29s/it]\n",
      " 64%|██████▍   | 64/100 [03:33<01:58,  3.30s/it]\n",
      " 64%|██████▍   | 64/100 [03:33<01:58,  3.30s/it]\n",
      " 65%|██████▌   | 65/100 [03:36<01:55,  3.30s/it]\n",
      " 65%|██████▌   | 65/100 [03:36<01:55,  3.30s/it]\n",
      " 66%|██████▌   | 66/100 [03:39<01:52,  3.30s/it]\n",
      " 66%|██████▌   | 66/100 [03:40<01:52,  3.30s/it]\n",
      " 67%|██████▋   | 67/100 [03:43<01:48,  3.30s/it]\n",
      " 67%|██████▋   | 67/100 [03:43<01:48,  3.30s/it]\n",
      " 68%|██████▊   | 68/100 [03:46<01:45,  3.30s/it]\n",
      " 68%|██████▊   | 68/100 [03:46<01:45,  3.31s/it]\n",
      " 69%|██████▉   | 69/100 [03:49<01:42,  3.30s/it]\n",
      " 69%|██████▉   | 69/100 [03:50<01:42,  3.31s/it]\n",
      " 70%|███████   | 70/100 [03:53<01:39,  3.30s/it]\n",
      " 70%|███████   | 70/100 [03:53<01:39,  3.31s/it]\n",
      " 71%|███████   | 71/100 [03:56<01:35,  3.31s/it]\n",
      " 71%|███████   | 71/100 [03:56<01:35,  3.31s/it]\n",
      " 72%|███████▏  | 72/100 [03:59<01:32,  3.31s/it]\n",
      " 72%|███████▏  | 72/100 [04:00<01:32,  3.31s/it]\n",
      " 73%|███████▎  | 73/100 [04:03<01:29,  3.31s/it]\n",
      " 73%|███████▎  | 73/100 [04:03<01:29,  3.31s/it]\n",
      " 74%|███████▍  | 74/100 [04:06<01:26,  3.31s/it]\n",
      " 74%|███████▍  | 74/100 [04:06<01:26,  3.31s/it]\n",
      " 75%|███████▌  | 75/100 [04:09<01:22,  3.31s/it]\n",
      " 75%|███████▌  | 75/100 [04:10<01:22,  3.31s/it]\n",
      " 76%|███████▌  | 76/100 [04:12<01:19,  3.30s/it]\n",
      " 76%|███████▌  | 76/100 [04:13<01:19,  3.31s/it]\n",
      " 77%|███████▋  | 77/100 [04:16<01:16,  3.31s/it]\n",
      " 77%|███████▋  | 77/100 [04:16<01:16,  3.31s/it]\n",
      " 78%|███████▊  | 78/100 [04:19<01:12,  3.31s/it]\n",
      " 78%|███████▊  | 78/100 [04:20<01:12,  3.31s/it]\n",
      " 79%|███████▉  | 79/100 [04:22<01:09,  3.32s/it]\n",
      " 79%|███████▉  | 79/100 [04:23<01:09,  3.30s/it]\n",
      " 80%|████████  | 80/100 [04:26<01:06,  3.33s/it]\n",
      " 80%|████████  | 80/100 [04:26<01:06,  3.30s/it]\n",
      " 81%|████████  | 81/100 [04:29<01:03,  3.33s/it]\n",
      " 81%|████████  | 81/100 [04:29<01:02,  3.30s/it]\n",
      " 82%|████████▏ | 82/100 [04:32<00:59,  3.33s/it]\n",
      " 82%|████████▏ | 82/100 [04:33<00:59,  3.30s/it]\n",
      " 83%|████████▎ | 83/100 [04:36<00:56,  3.32s/it]\n",
      " 83%|████████▎ | 83/100 [04:36<00:55,  3.29s/it]\n",
      " 84%|████████▍ | 84/100 [04:39<00:53,  3.31s/it]\n",
      " 84%|████████▍ | 84/100 [04:39<00:52,  3.29s/it]\n",
      " 85%|████████▌ | 85/100 [04:42<00:49,  3.31s/it]\n",
      " 85%|████████▌ | 85/100 [04:43<00:49,  3.29s/it]\n",
      " 86%|████████▌ | 86/100 [04:46<00:46,  3.30s/it]\n",
      " 86%|████████▌ | 86/100 [04:46<00:46,  3.29s/it]\n",
      " 87%|████████▋ | 87/100 [04:49<00:42,  3.30s/it]\n",
      " 87%|████████▋ | 87/100 [04:49<00:42,  3.30s/it]\n",
      " 88%|████████▊ | 88/100 [04:52<00:39,  3.30s/it]\n",
      " 88%|████████▊ | 88/100 [04:52<00:39,  3.30s/it]\n",
      " 89%|████████▉ | 89/100 [04:55<00:36,  3.30s/it]\n",
      " 89%|████████▉ | 89/100 [04:56<00:36,  3.31s/it]\n",
      " 90%|█████████ | 90/100 [04:59<00:32,  3.30s/it]\n",
      " 90%|█████████ | 90/100 [04:59<00:33,  3.30s/it]\n",
      " 91%|█████████ | 91/100 [05:02<00:29,  3.30s/it]\n",
      " 91%|█████████ | 91/100 [05:02<00:29,  3.31s/it]\n",
      " 92%|█████████▏| 92/100 [05:05<00:26,  3.31s/it]\n",
      " 92%|█████████▏| 92/100 [05:06<00:26,  3.32s/it]\n",
      " 93%|█████████▎| 93/100 [05:09<00:23,  3.31s/it]\n",
      " 93%|█████████▎| 93/100 [05:09<00:23,  3.31s/it]\n",
      " 94%|█████████▍| 94/100 [05:12<00:19,  3.30s/it]\n",
      " 94%|█████████▍| 94/100 [05:12<00:19,  3.31s/it]\n",
      " 95%|█████████▌| 95/100 [05:15<00:16,  3.30s/it]\n",
      " 95%|█████████▌| 95/100 [05:16<00:16,  3.31s/it]\n",
      " 96%|█████████▌| 96/100 [05:19<00:13,  3.30s/it]\n",
      " 96%|█████████▌| 96/100 [05:19<00:13,  3.32s/it]\n",
      " 97%|█████████▋| 97/100 [05:22<00:09,  3.30s/it]\n",
      " 97%|█████████▋| 97/100 [05:22<00:09,  3.33s/it]\n",
      " 98%|█████████▊| 98/100 [05:25<00:06,  3.32s/it]\n",
      " 98%|█████████▊| 98/100 [05:26<00:06,  3.32s/it]\n",
      " 99%|█████████▉| 99/100 [05:29<00:03,  3.32s/it]\n",
      " 99%|█████████▉| 99/100 [05:29<00:03,  3.32s/it]\n",
      "100%|██████████| 100/100 [05:32<00:00,  3.32s/it]\n",
      "100%|██████████| 100/100 [05:32<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2502)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2502)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2502)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2502)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2502)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2502)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2502)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2502)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:04<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=2503)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:05<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "#\"\"\" Finetuning a 🤗 Transformers model for sequence classification.\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from typing import Dict, Any\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5001')\n",
    "\n",
    "import datasets\n",
    "import ray\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset, load_metric\n",
    "from ray.train import Trainer\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Finetune a transformers model on a text classification task\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Ignore this!\",\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--train_file\",\n",
    "        type=str,\n",
    "        default=\"data/train/part-algo-1-womens_clothing_ecommerce_reviews.csv\",\n",
    "        help=\"A csv or a json file containing the training data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_file\",\n",
    "        type=str,\n",
    "        default=\"data/validation/part-algo-1-womens_clothing_ecommerce_reviews.csv\",\n",
    "        help=\"A csv or a json file containing the validation data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        help=(\n",
    "            \"The maximum total input sequence length after tokenization. \"\n",
    "            \"Sequences longer than this will be truncated, sequences shorter \"\n",
    "            \"will be padded if `--pad_to_max_lengh` is passed.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pad_to_max_length\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic \"\n",
    "        \"padding is used.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        help=\"Path to pretrained model or model identifier from \"\n",
    "        \"huggingface.co/models.\",\n",
    "        default=\"roberta-base\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_slow_tokenizer\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, will use a slow tokenizer (not backed by the 🤗 \"\n",
    "        \"Tokenizers library).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_train_batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Batch size (per device) for the training dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_eval_batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Batch size (per device) for the evaluation dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-5,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weight_decay\", type=float, default=0.0, help=\"Weight decay to use.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Total number of training epochs to perform.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"Total number of training steps to perform. If provided, \"\n",
    "        \"overrides num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a \"\n",
    "        \"backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler_type\",\n",
    "        type=SchedulerType,\n",
    "        default=\"linear\",\n",
    "        help=\"The scheduler type to use.\",\n",
    "        choices=[\n",
    "            \"linear\",\n",
    "            \"cosine\",\n",
    "            \"cosine_with_restarts\",\n",
    "            \"polynomial\",\n",
    "            \"constant\",\n",
    "            \"constant_with_warmup\",\n",
    "        ],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_warmup_steps\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Number of steps for the warmup in the lr scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", type=str, default=None, help=\"Where to store the final model.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=None, help=\"A seed for reproducible training.\"\n",
    "    )\n",
    "\n",
    "    # Ray arguments.\n",
    "    parser.add_argument(\n",
    "        \"--start_local\", action=\"store_true\", help=\"Starts Ray on local machine.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--address\", \n",
    "        type=str, \n",
    "        default=\"127.0.0.1:6379\", \n",
    "        help=\"Ray address to connect to.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_workers\", \n",
    "        type=int, \n",
    "        default=2, \n",
    "        help=\"Number of workers to use.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_gpu\", action=\"store_true\", help=\"If training should be done on GPUs.\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Sanity checks\n",
    "    if (\n",
    "#        args.task_name is None\n",
    "        args.train_file is None\n",
    "        and args.validation_file is None\n",
    "    ):\n",
    "        raise ValueError(\"Need a training/validation file.\")\n",
    "    else:\n",
    "        if args.train_file is not None:\n",
    "            extension = args.train_file.split(\".\")[-1]\n",
    "            assert extension in [\n",
    "                \"csv\",\n",
    "                \"json\",\n",
    "            ], \"`train_file` should be a csv or a json file.\"\n",
    "        if args.validation_file is not None:\n",
    "            extension = args.validation_file.split(\".\")[-1]\n",
    "            assert extension in [\n",
    "                \"csv\",\n",
    "                \"json\",\n",
    "            ], \"`validation_file` should be a csv or a json file.\"\n",
    "\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_func(config: Dict[str, Any]):\n",
    "    args = config[\"args\"]\n",
    "    # Initialize the accelerator. We will let the accelerator handle device\n",
    "    # placement for us in this example.\n",
    "    accelerator = Accelerator()\n",
    "    # Make one log on every process with the configuration for debugging.\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.ERROR,\n",
    "    )\n",
    "    logger.info(accelerator.state)\n",
    "\n",
    "    # Setup logging, we only want one process per machine to log things on\n",
    "    # the screen. accelerator.is_local_main_process is only True for one\n",
    "    # process per machine.\n",
    "    logger.setLevel(\n",
    "        logging.ERROR if accelerator.is_local_main_process else logging.ERROR\n",
    "    )\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # If passed along, set the training seed now.\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON training and\n",
    "    # evaluation files (see below) or specify a GLUE benchmark task (the\n",
    "    # dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "    # For CSV/JSON files, this script will use as labels the column called\n",
    "    # 'label' and as pair of sentences the sentences in columns called\n",
    "    # 'sentence1' and 'sentence2' if such column exists or the first two\n",
    "    # columns not named label if at least two columns are provided.\n",
    "\n",
    "    # If the CSVs/JSONs contain only one non-label column, the script does\n",
    "    # single sentence classification on this single column. You can easily\n",
    "    # tweak this behavior (see below)\n",
    "\n",
    "    # In distributed training, the load_dataset function guarantee that only\n",
    "    # one local process can concurrently download the dataset.\n",
    "#    if args.task_name is not None:\n",
    "#        # Downloading and loading a dataset from the hub.\n",
    "#        raw_datasets = load_dataset(\"glue\", args.task_name)\n",
    "#    else:\n",
    "        # Loading the dataset from local csv or json file.\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    extension = (\n",
    "        args.train_file if args.train_file is not None else args.valid_file\n",
    "    ).split(\".\")[-1]\n",
    "\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "\n",
    "    label_list = raw_datasets[\"train\"].unique(\"sentiment\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # In distributed training, the .from_pretrained methods guarantee that\n",
    "    # only one local process can concurrently download model & vocab.\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        args.model_name_or_path, num_labels=num_labels, \n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path, use_fast=not args.use_slow_tokenizer\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Preprocessing the datasets\n",
    "    sentence1_key, sentence2_key = \"review_body\", None\n",
    "\n",
    "    # Some models have set the order of the labels to use,\n",
    "    # so let's make sure we do use it.\n",
    "    label_to_id = None\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "    if label_to_id is not None:\n",
    "        model.config.label2id = label_to_id\n",
    "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "    padding = \"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        texts = (\n",
    "            (examples[sentence1_key],)\n",
    "            if sentence2_key is None\n",
    "            else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(\n",
    "            *texts, padding=padding, max_length=args.max_length, truncation=True\n",
    "        )\n",
    "\n",
    "        if \"sentiment\" in examples:\n",
    "            if label_to_id is not None:\n",
    "                # Map labels to IDs (not necessary for GLUE tasks)\n",
    "                result[\"labels\"] = [\n",
    "                    label_to_id[l] for l in examples[\"sentiment\"]  # noqa:E741\n",
    "                ]\n",
    "            else:\n",
    "                # In all cases, rename the column to labels because the model\n",
    "                # will expect that.\n",
    "                result[\"labels\"] = examples[\"sentiment\"]\n",
    "\n",
    "        return result\n",
    "\n",
    "    processed_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"validation\"]\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    # DataLoaders creation:\n",
    "    if args.pad_to_max_length:\n",
    "        # If padding was already done ot max length, we use the default data\n",
    "        # collator that will just convert everything to tensors.\n",
    "        data_collator = default_data_collator\n",
    "    else:\n",
    "        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for\n",
    "        # us (by padding to the maximum length of the samples passed). When\n",
    "        # using mixed precision, we add `pad_to_multiple_of=8` to pad all\n",
    "        # tensors to multiple of 8s, which will enable the use of Tensor\n",
    "        # Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "            tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "        )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=args.per_device_train_batch_size,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=args.per_device_eval_batch_size,\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    # Split weights in two groups, one with weight decay and the other not.\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "#    model, optimizer, train_dataloader = accelerator.prepare(\n",
    "#        model, optimizer, train_dataloader\n",
    "#    )\n",
    "    # Note -> the training dataloader needs to be prepared before we grab\n",
    "    # his length below (cause its length will be shorter in multiprocess)\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / args.gradient_accumulation_steps\n",
    "    )\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    else:\n",
    "        args.num_train_epochs = math.ceil(\n",
    "            args.max_train_steps / num_update_steps_per_epoch\n",
    "        )\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=args.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.num_warmup_steps,\n",
    "        num_training_steps=args.max_train_steps,\n",
    "    )\n",
    "\n",
    "    # Get the metric function\n",
    "#    if args.task_name is not None:\n",
    "#        metric = load_metric(\"glue\", args.task_name)\n",
    "#    else:\n",
    "    metric = load_metric(\"accuracy\")\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = (\n",
    "        args.per_device_train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * args.gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num epochs = {args.num_train_epochs}\")\n",
    "    logger.info(\n",
    "        f\"  Instantaneous batch size per device =\"\n",
    "        f\" {args.per_device_train_batch_size}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"  Total train batch size (w. parallel, distributed & accumulation) \"\n",
    "        f\"= {total_batch_size}\"\n",
    "    )\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(\n",
    "        range(args.max_train_steps), disable=not accelerator.is_local_main_process\n",
    "    )\n",
    "    completed_steps = 0\n",
    "\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if (\n",
    "                step % args.gradient_accumulation_steps == 0\n",
    "                or step == len(train_dataloader) - 1\n",
    "            ):\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            predictions = (\n",
    "                outputs.logits.argmax(dim=-1)\n",
    "#                if not is_regression\n",
    "#                else outputs.logits.squeeze()\n",
    "            )\n",
    "            metric.add_batch(\n",
    "                predictions=accelerator.gather(predictions),\n",
    "                references=accelerator.gather(batch[\"labels\"]),\n",
    "            )\n",
    "\n",
    "        mlflow.log_param(\"batch_size\", args.per_device_train_batch_size)\n",
    "        mlflow.log_param(\"learning_rate\", args.learning_rate)\n",
    "        mlflow.log_param(\"weight_decay\", args.weight_decay)\n",
    "        mlflow.log_param(\"max_length\", args.max_length)\n",
    "\n",
    "        eval_metric = metric.compute()\n",
    "        mlflow.log_metric(\"accuracy\", eval_metric['accuracy'])\n",
    "        \n",
    "        logger.info(f\"epoch {epoch}: {eval_metric}\")\n",
    "\n",
    "    if args.output_dir is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    config = {\"args\": args}\n",
    "    args.use_gpu = False\n",
    "\n",
    "    if args.start_local or args.address or args.num_workers > 1 or args.use_gpu:\n",
    "        if args.start_local:\n",
    "            # Start a local Ray runtime.\n",
    "            ray.init(num_cpus=args.num_workers)\n",
    "        else:\n",
    "            # Connect to a Ray cluster for distributed training.\n",
    "            ray.init(address=args.address)\n",
    "        trainer = Trainer(\"torch\", num_workers=args.num_workers, use_gpu=args.use_gpu,\n",
    "                          resources_per_worker={'CPU': 4, 'GPU': 0})\n",
    "        trainer.start()\n",
    "        trainer.run(train_func, config)\n",
    "    else:\n",
    "        # Run training locally.\n",
    "        train_func(config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b409397-9ae1-43f9-a238-eab3c6c2141e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
