{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf5dc9c-5f7c-45dd-a702-89fb0ecf58e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 07:57:30,115\tINFO trainer.py:243 -- Trainer logs will be logged in: /home/ray/ray_results/train_2022-06-24_07-57-30\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m 2022-06-24 07:57:35,279\tINFO torch.py:347 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "2022-06-24 07:57:35,350\tINFO trainer.py:249 -- Run results will be logged in: /home/ray/ray_results/train_2022-06-24_07-57-30/run_001\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m 2022-06-24 07:57:35,305\tINFO torch.py:347 -- Setting up process group for: env:// [rank=1, world_size=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m Downloading and preparing dataset csv/default to /home/ray/.cache/huggingface/datasets/csv/default-3a58b73918809981/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m Downloading and preparing dataset csv/default to /home/ray/.cache/huggingface/datasets/csv/default-3a58b73918809981/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m Dataset csv downloaded and prepared to /home/ray/.cache/huggingface/datasets/csv/default-3a58b73918809981/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m Dataset csv downloaded and prepared to /home/ray/.cache/huggingface/datasets/csv/default-3a58b73918809981/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 5486.34it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 896.51it/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 4739.33it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1169.47it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 730.14it/s].7)\u001b[0m \n",
      "100%|██████████| 2/2 [00:00<00:00, 757.85it/s].32)\u001b[0m \n",
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 440kB/s]\n",
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 460kB/s]\n",
      "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]\n",
      "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]\n",
      "Downloading:   4%|▎         | 32.0k/878k [00:00<00:03, 265kB/s]\n",
      "Downloading:   5%|▍         | 40.0k/878k [00:00<00:02, 311kB/s]\n",
      "Downloading:  11%|█▏        | 100k/878k [00:00<00:01, 439kB/s] \n",
      "Downloading:  12%|█▏        | 108k/878k [00:00<00:01, 437kB/s] \n",
      "Downloading: 100%|██████████| 878k/878k [00:00<00:00, 2.02MB/s]\n",
      "Downloading:  25%|██▌       | 220k/878k [00:00<00:01, 631kB/s]\n",
      "Downloading:  34%|███▍      | 300k/878k [00:00<00:00, 625kB/s]\n",
      "Downloading:  63%|██████▎   | 556k/878k [00:00<00:00, 1.11MB/s]\n",
      "Downloading: 100%|██████████| 878k/878k [00:00<00:00, 1.22MB/s]\n",
      "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]\n",
      "Downloading:   6%|▋         | 28.0k/446k [00:00<00:01, 222kB/s]\n",
      "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]\n",
      "Downloading:  30%|██▉       | 133k/446k [00:00<00:00, 572kB/s] \n",
      "Downloading:   8%|▊         | 36.0k/446k [00:00<00:01, 281kB/s]\n",
      "Downloading:  44%|████▍     | 197k/446k [00:00<00:00, 545kB/s]\n",
      "Downloading:  69%|██████▉   | 309k/446k [00:00<00:00, 673kB/s]\n",
      "Downloading:  19%|█▉        | 85.0k/446k [00:00<00:01, 340kB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 694kB/s]\n",
      "Downloading:  44%|████▍     | 197k/446k [00:00<00:00, 582kB/s] \n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 863kB/s] \n",
      "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]\n",
      "Downloading:   3%|▎         | 37.0k/1.29M [00:00<00:04, 293kB/s]\n",
      "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]\n",
      "Downloading:   7%|▋         | 91.0k/1.29M [00:00<00:03, 371kB/s]\n",
      "Downloading:   5%|▍         | 64.0k/1.29M [00:00<00:02, 513kB/s]\n",
      "Downloading:  13%|█▎        | 171k/1.29M [00:00<00:02, 489kB/s] \n",
      "Downloading:  13%|█▎        | 176k/1.29M [00:00<00:01, 729kB/s] \n",
      "Downloading:  36%|███▌      | 475k/1.29M [00:00<00:00, 1.24MB/s]\n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 2.23MB/s]\n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 2.91MB/s]\n",
      "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]\n",
      "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]\n",
      "Downloading:   0%|          | 1.31M/478M [00:00<00:36, 13.8MB/s]\n",
      "Downloading:   0%|          | 772k/478M [00:00<01:04, 7.76MB/s]\n",
      "Downloading:   1%|          | 5.26M/478M [00:00<00:16, 30.0MB/s]\n",
      "Downloading:   0%|          | 1.49M/478M [00:00<01:04, 7.74MB/s]\n",
      "Downloading:   2%|▏         | 9.49M/478M [00:00<00:13, 36.6MB/s]\n",
      "Downloading:   0%|          | 2.25M/478M [00:00<01:04, 7.76MB/s]\n",
      "Downloading:   3%|▎         | 13.7M/478M [00:00<00:12, 39.7MB/s]\n",
      "Downloading:   1%|          | 3.00M/478M [00:00<01:04, 7.75MB/s]\n",
      "Downloading:   4%|▍         | 18.0M/478M [00:00<00:11, 41.5MB/s]\n",
      "Downloading:   1%|          | 3.74M/478M [00:00<01:04, 7.73MB/s]\n",
      "Downloading:   5%|▍         | 22.3M/478M [00:00<00:11, 42.6MB/s]\n",
      "Downloading:   1%|          | 4.97M/478M [00:00<00:52, 9.47MB/s]\n",
      "Downloading:   6%|▌         | 26.3M/478M [00:00<00:11, 40.9MB/s]\n",
      "Downloading:   3%|▎         | 13.1M/478M [00:00<00:17, 27.6MB/s]\n",
      "Downloading:   6%|▋         | 30.2M/478M [00:00<00:11, 40.0MB/s]\n",
      "Downloading:   4%|▎         | 17.4M/478M [00:00<00:14, 33.1MB/s]\n",
      "Downloading:   7%|▋         | 34.5M/478M [00:00<00:11, 41.5MB/s]\n",
      "Downloading:   5%|▍         | 21.8M/478M [00:01<00:12, 37.0MB/s]\n",
      "Downloading:   8%|▊         | 38.8M/478M [00:01<00:10, 42.7MB/s]\n",
      "Downloading:   5%|▌         | 26.1M/478M [00:01<00:11, 39.6MB/s]\n",
      "Downloading:   9%|▉         | 43.2M/478M [00:01<00:10, 43.6MB/s]\n",
      "Downloading:   6%|▋         | 30.5M/478M [00:01<00:11, 41.6MB/s]\n",
      "Downloading:  10%|▉         | 47.6M/478M [00:01<00:10, 44.2MB/s]\n",
      "Downloading:   7%|▋         | 34.9M/478M [00:01<00:10, 42.9MB/s]\n",
      "Downloading:  11%|█         | 51.9M/478M [00:01<00:10, 44.6MB/s]\n",
      "Downloading:   8%|▊         | 39.3M/478M [00:01<00:10, 43.9MB/s]\n",
      "Downloading:  12%|█▏        | 56.2M/478M [00:01<00:09, 44.7MB/s]\n",
      "Downloading:   9%|▉         | 43.8M/478M [00:01<00:10, 44.7MB/s]\n",
      "Downloading:  13%|█▎        | 60.5M/478M [00:01<00:09, 44.9MB/s]\n",
      "Downloading:  10%|█         | 48.2M/478M [00:01<00:09, 45.3MB/s]\n",
      "Downloading:  14%|█▎        | 64.9M/478M [00:01<00:09, 45.1MB/s]\n",
      "Downloading:  11%|█         | 52.6M/478M [00:01<00:09, 45.5MB/s]\n",
      "Downloading:  14%|█▍        | 69.2M/478M [00:01<00:09, 45.3MB/s]\n",
      "Downloading:  12%|█▏        | 57.0M/478M [00:01<00:09, 45.7MB/s]\n",
      "Downloading:  15%|█▌        | 73.6M/478M [00:01<00:09, 45.5MB/s]\n",
      "Downloading:  13%|█▎        | 61.4M/478M [00:01<00:09, 45.9MB/s]\n",
      "Downloading:  16%|█▋        | 78.0M/478M [00:01<00:09, 45.4MB/s]\n",
      "Downloading:  14%|█▍        | 65.8M/478M [00:02<00:09, 45.9MB/s]\n",
      "Downloading:  17%|█▋        | 82.3M/478M [00:02<00:09, 45.4MB/s]\n",
      "Downloading:  15%|█▍        | 70.2M/478M [00:02<00:09, 45.9MB/s]\n",
      "Downloading:  18%|█▊        | 86.6M/478M [00:02<00:09, 45.4MB/s]\n",
      "Downloading:  16%|█▌        | 74.6M/478M [00:02<00:09, 45.9MB/s]\n",
      "Downloading:  19%|█▉        | 91.0M/478M [00:02<00:08, 45.4MB/s]\n",
      "Downloading:  17%|█▋        | 78.9M/478M [00:02<00:09, 45.8MB/s]\n",
      "Downloading:  20%|█▉        | 95.3M/478M [00:02<00:08, 45.3MB/s]\n",
      "Downloading:  17%|█▋        | 83.3M/478M [00:02<00:09, 45.8MB/s]\n",
      "Downloading:  21%|██        | 99.6M/478M [00:02<00:08, 45.3MB/s]\n",
      "Downloading:  18%|█▊        | 87.7M/478M [00:02<00:08, 45.8MB/s]\n",
      "Downloading:  22%|██▏       | 104M/478M [00:02<00:08, 45.4MB/s] \n",
      "Downloading:  19%|█▉        | 92.1M/478M [00:02<00:08, 45.9MB/s]\n",
      "Downloading:  23%|██▎       | 108M/478M [00:02<00:08, 45.3MB/s]\n",
      "Downloading:  20%|██        | 96.5M/478M [00:02<00:08, 45.7MB/s]\n",
      "Downloading:  24%|██▎       | 113M/478M [00:02<00:08, 45.5MB/s]\n",
      "Downloading:  21%|██        | 101M/478M [00:02<00:08, 45.7MB/s] \n",
      "Downloading:  24%|██▍       | 117M/478M [00:02<00:08, 45.5MB/s]\n",
      "Downloading:  22%|██▏       | 105M/478M [00:02<00:08, 45.7MB/s]\n",
      "Downloading:  25%|██▌       | 121M/478M [00:02<00:08, 45.5MB/s]\n",
      "Downloading:  23%|██▎       | 110M/478M [00:03<00:08, 45.5MB/s]\n",
      "Downloading:  26%|██▋       | 126M/478M [00:03<00:08, 45.5MB/s]\n",
      "Downloading:  24%|██▍       | 114M/478M [00:03<00:08, 45.4MB/s]\n",
      "Downloading:  27%|██▋       | 130M/478M [00:03<00:08, 45.5MB/s]\n",
      "Downloading:  25%|██▍       | 118M/478M [00:03<00:08, 45.5MB/s]\n",
      "Downloading:  28%|██▊       | 134M/478M [00:03<00:07, 45.6MB/s]\n",
      "Downloading:  26%|██▌       | 123M/478M [00:03<00:08, 45.3MB/s]\n",
      "Downloading:  29%|██▉       | 139M/478M [00:03<00:07, 45.6MB/s]\n",
      "Downloading:  27%|██▋       | 127M/478M [00:03<00:08, 45.3MB/s]\n",
      "Downloading:  30%|██▉       | 143M/478M [00:03<00:07, 45.5MB/s]\n",
      "Downloading:  27%|██▋       | 131M/478M [00:03<00:08, 45.3MB/s]\n",
      "Downloading:  31%|███       | 147M/478M [00:03<00:07, 45.6MB/s]\n",
      "Downloading:  28%|██▊       | 136M/478M [00:03<00:07, 45.3MB/s]\n",
      "Downloading:  32%|███▏      | 152M/478M [00:03<00:07, 44.7MB/s]\n",
      "Downloading:  29%|██▉       | 140M/478M [00:03<00:07, 45.2MB/s]\n",
      "Downloading:  33%|███▎      | 156M/478M [00:03<00:07, 44.8MB/s]\n",
      "Downloading:  30%|███       | 144M/478M [00:03<00:07, 45.2MB/s]\n",
      "Downloading:  34%|███▎      | 160M/478M [00:03<00:07, 44.9MB/s]\n",
      "Downloading:  31%|███       | 149M/478M [00:03<00:07, 45.4MB/s]\n",
      "Downloading:  34%|███▍      | 165M/478M [00:03<00:07, 45.0MB/s]\n",
      "Downloading:  32%|███▏      | 153M/478M [00:04<00:07, 45.3MB/s]\n",
      "Downloading:  35%|███▌      | 169M/478M [00:04<00:07, 45.0MB/s]\n",
      "Downloading:  33%|███▎      | 157M/478M [00:04<00:07, 45.3MB/s]\n",
      "Downloading:  36%|███▋      | 173M/478M [00:04<00:07, 45.3MB/s]\n",
      "Downloading:  34%|███▍      | 162M/478M [00:04<00:07, 45.4MB/s]\n",
      "Downloading:  38%|███▊      | 182M/478M [00:04<00:06, 45.7MB/s]\n",
      "Downloading:  35%|███▍      | 166M/478M [00:04<00:07, 45.3MB/s]\n",
      "Downloading:  39%|███▉      | 187M/478M [00:04<00:06, 45.8MB/s]\n",
      "Downloading:  36%|███▌      | 170M/478M [00:04<00:07, 45.1MB/s]\n",
      "Downloading:  40%|███▉      | 191M/478M [00:04<00:06, 46.0MB/s]\n",
      "Downloading:  37%|███▋      | 175M/478M [00:04<00:07, 44.4MB/s]\n",
      "Downloading:  41%|████      | 195M/478M [00:04<00:06, 46.1MB/s]\n",
      "Downloading:  37%|███▋      | 179M/478M [00:04<00:07, 43.7MB/s]\n",
      "Downloading:  42%|████▏     | 200M/478M [00:04<00:06, 46.3MB/s]\n",
      "Downloading:  38%|███▊      | 183M/478M [00:04<00:07, 44.2MB/s]\n",
      "Downloading:  43%|████▎     | 204M/478M [00:04<00:06, 46.1MB/s]\n",
      "Downloading:  39%|███▉      | 187M/478M [00:04<00:06, 44.6MB/s]\n",
      "Downloading:  44%|████▎     | 209M/478M [00:04<00:06, 46.0MB/s]\n",
      "Downloading:  40%|████      | 192M/478M [00:04<00:06, 45.1MB/s]\n",
      "Downloading:  45%|████▍     | 213M/478M [00:05<00:06, 46.0MB/s]\n",
      "Downloading:  41%|████      | 196M/478M [00:05<00:06, 45.2MB/s]\n",
      "Downloading:  46%|████▌     | 218M/478M [00:05<00:05, 46.0MB/s]\n",
      "Downloading:  42%|████▏     | 200M/478M [00:05<00:06, 43.9MB/s]\n",
      "Downloading:  46%|████▋     | 222M/478M [00:05<00:05, 46.0MB/s]\n",
      "Downloading:  43%|████▎     | 205M/478M [00:05<00:06, 44.5MB/s]\n",
      "Downloading:  47%|████▋     | 226M/478M [00:05<00:05, 45.8MB/s]\n",
      "Downloading:  44%|████▍     | 209M/478M [00:05<00:06, 44.9MB/s]\n",
      "Downloading:  48%|████▊     | 231M/478M [00:05<00:05, 45.7MB/s]\n",
      "Downloading:  45%|████▍     | 214M/478M [00:05<00:06, 44.6MB/s]\n",
      "Downloading:  49%|████▉     | 235M/478M [00:05<00:05, 45.7MB/s]\n",
      "Downloading:  46%|████▌     | 218M/478M [00:05<00:06, 44.9MB/s]\n",
      "Downloading:  50%|█████     | 239M/478M [00:05<00:05, 45.8MB/s]\n",
      "Downloading:  46%|████▋     | 222M/478M [00:05<00:05, 45.0MB/s]\n",
      "Downloading:  51%|█████     | 244M/478M [00:05<00:05, 45.7MB/s]\n",
      "Downloading:  47%|████▋     | 226M/478M [00:05<00:05, 45.1MB/s]\n",
      "Downloading:  52%|█████▏    | 248M/478M [00:05<00:05, 45.6MB/s]\n",
      "Downloading:  48%|████▊     | 231M/478M [00:05<00:05, 45.1MB/s]\n",
      "Downloading:  53%|█████▎    | 252M/478M [00:05<00:05, 45.6MB/s]\n",
      "Downloading:  49%|████▉     | 235M/478M [00:05<00:05, 45.0MB/s]\n",
      "Downloading:  54%|█████▎    | 257M/478M [00:06<00:05, 45.7MB/s]\n",
      "Downloading:  50%|█████     | 239M/478M [00:06<00:05, 44.1MB/s]\n",
      "Downloading:  55%|█████▍    | 261M/478M [00:06<00:04, 45.7MB/s]\n",
      "Downloading:  51%|█████     | 244M/478M [00:06<00:05, 43.8MB/s]\n",
      "Downloading:  56%|█████▌    | 266M/478M [00:06<00:04, 45.8MB/s]\n",
      "Downloading:  52%|█████▏    | 248M/478M [00:06<00:05, 44.2MB/s]\n",
      "Downloading:  56%|█████▋    | 270M/478M [00:06<00:04, 45.8MB/s]\n",
      "Downloading:  53%|█████▎    | 252M/478M [00:06<00:05, 44.7MB/s]\n",
      "Downloading:  57%|█████▋    | 274M/478M [00:06<00:04, 45.7MB/s]\n",
      "Downloading:  54%|█████▎    | 257M/478M [00:06<00:05, 44.9MB/s]\n",
      "Downloading:  58%|█████▊    | 279M/478M [00:06<00:04, 45.8MB/s]\n",
      "Downloading:  55%|█████▍    | 261M/478M [00:06<00:05, 45.2MB/s]\n",
      "Downloading:  56%|█████▌    | 265M/478M [00:06<00:04, 45.5MB/s]\n",
      "Downloading:  59%|█████▉    | 283M/478M [00:06<00:04, 43.3MB/s]\n",
      "Downloading:  56%|█████▋    | 270M/478M [00:06<00:04, 45.7MB/s]\n",
      "Downloading:  60%|██████    | 287M/478M [00:06<00:04, 44.0MB/s]\n",
      "Downloading:  57%|█████▋    | 274M/478M [00:06<00:04, 45.9MB/s]\n",
      "Downloading:  61%|██████    | 292M/478M [00:06<00:04, 44.6MB/s]\n",
      "Downloading:  58%|█████▊    | 279M/478M [00:06<00:04, 46.0MB/s]\n",
      "Downloading:  62%|██████▏   | 296M/478M [00:06<00:04, 45.1MB/s]\n",
      "Downloading:  59%|█████▉    | 283M/478M [00:07<00:04, 46.0MB/s]\n",
      "Downloading:  63%|██████▎   | 301M/478M [00:07<00:04, 45.4MB/s]\n",
      "Downloading:  60%|██████    | 287M/478M [00:07<00:04, 45.3MB/s]\n",
      "Downloading:  64%|██████▍   | 305M/478M [00:07<00:03, 45.7MB/s]\n",
      "Downloading:  61%|██████    | 292M/478M [00:07<00:04, 44.5MB/s]\n",
      "Downloading:  65%|██████▍   | 309M/478M [00:07<00:03, 45.4MB/s]\n",
      "Downloading:  62%|██████▏   | 296M/478M [00:07<00:04, 45.3MB/s]\n",
      "Downloading:  66%|██████▌   | 314M/478M [00:07<00:03, 45.7MB/s]\n",
      "Downloading:  63%|██████▎   | 301M/478M [00:07<00:04, 45.7MB/s]\n",
      "Downloading:  67%|██████▋   | 318M/478M [00:07<00:03, 45.6MB/s]\n",
      "Downloading:  64%|██████▍   | 305M/478M [00:07<00:03, 46.2MB/s]\n",
      "Downloading:  67%|██████▋   | 323M/478M [00:07<00:03, 45.7MB/s]\n",
      "Downloading:  65%|██████▍   | 310M/478M [00:07<00:03, 46.6MB/s]\n",
      "Downloading:  68%|██████▊   | 327M/478M [00:07<00:03, 45.7MB/s]\n",
      "Downloading:  66%|██████▌   | 314M/478M [00:07<00:03, 46.8MB/s]\n",
      "Downloading:  69%|██████▉   | 331M/478M [00:07<00:03, 45.8MB/s]\n",
      "Downloading:  67%|██████▋   | 319M/478M [00:07<00:03, 46.9MB/s]\n",
      "Downloading:  70%|███████   | 336M/478M [00:07<00:03, 45.8MB/s]\n",
      "Downloading:  68%|██████▊   | 323M/478M [00:07<00:03, 46.8MB/s]\n",
      "Downloading:  71%|███████   | 340M/478M [00:07<00:03, 45.9MB/s]\n",
      "Downloading:  69%|██████▊   | 328M/478M [00:08<00:03, 46.8MB/s]\n",
      "Downloading:  72%|███████▏  | 345M/478M [00:08<00:03, 45.8MB/s]\n",
      "Downloading:  70%|██████▉   | 332M/478M [00:08<00:03, 46.9MB/s]\n",
      "Downloading:  73%|███████▎  | 349M/478M [00:08<00:02, 45.7MB/s]\n",
      "Downloading:  70%|███████   | 337M/478M [00:08<00:03, 46.9MB/s]\n",
      "Downloading:  74%|███████▍  | 353M/478M [00:08<00:02, 45.4MB/s]\n",
      "Downloading:  71%|███████▏  | 341M/478M [00:08<00:03, 46.8MB/s]\n",
      "Downloading:  75%|███████▍  | 358M/478M [00:08<00:02, 45.3MB/s]\n",
      "Downloading:  72%|███████▏  | 346M/478M [00:08<00:02, 46.7MB/s]\n",
      "Downloading:  76%|███████▌  | 362M/478M [00:08<00:02, 45.3MB/s]\n",
      "Downloading:  73%|███████▎  | 350M/478M [00:08<00:02, 46.6MB/s]\n",
      "Downloading:  77%|███████▋  | 366M/478M [00:08<00:02, 45.2MB/s]\n",
      "Downloading:  74%|███████▍  | 355M/478M [00:08<00:02, 46.6MB/s]\n",
      "Downloading:  78%|███████▊  | 371M/478M [00:08<00:02, 44.4MB/s]\n",
      "Downloading:  75%|███████▌  | 359M/478M [00:08<00:02, 46.6MB/s]\n",
      "Downloading:  78%|███████▊  | 375M/478M [00:08<00:02, 43.4MB/s]\n",
      "Downloading:  76%|███████▌  | 363M/478M [00:08<00:02, 46.6MB/s]\n",
      "Downloading:  79%|███████▉  | 379M/478M [00:08<00:02, 43.9MB/s]\n",
      "Downloading:  77%|███████▋  | 368M/478M [00:08<00:02, 46.6MB/s]\n",
      "Downloading:  80%|████████  | 383M/478M [00:08<00:02, 44.2MB/s]\n",
      "Downloading:  78%|███████▊  | 372M/478M [00:09<00:02, 46.5MB/s]\n",
      "Downloading:  81%|████████  | 388M/478M [00:09<00:02, 44.6MB/s]\n",
      "Downloading:  79%|███████▉  | 377M/478M [00:09<00:02, 46.4MB/s]\n",
      "Downloading:  82%|████████▏ | 392M/478M [00:09<00:02, 44.8MB/s]\n",
      "Downloading:  80%|███████▉  | 381M/478M [00:09<00:02, 46.3MB/s]\n",
      "Downloading:  83%|████████▎ | 396M/478M [00:09<00:01, 45.0MB/s]\n",
      "Downloading:  81%|████████  | 386M/478M [00:09<00:02, 46.2MB/s]\n",
      "Downloading:  84%|████████▍ | 401M/478M [00:09<00:01, 44.9MB/s]\n",
      "Downloading:  82%|████████▏ | 390M/478M [00:09<00:01, 46.4MB/s]\n",
      "Downloading:  85%|████████▍ | 405M/478M [00:09<00:01, 44.9MB/s]\n",
      "Downloading:  83%|████████▎ | 394M/478M [00:09<00:01, 46.2MB/s]\n",
      "Downloading:  86%|████████▌ | 409M/478M [00:09<00:01, 45.0MB/s]\n",
      "Downloading:  83%|████████▎ | 399M/478M [00:09<00:01, 46.4MB/s]\n",
      "Downloading:  87%|████████▋ | 414M/478M [00:09<00:01, 45.2MB/s]\n",
      "Downloading:  84%|████████▍ | 403M/478M [00:09<00:01, 45.4MB/s]\n",
      "Downloading:  87%|████████▋ | 418M/478M [00:09<00:01, 45.2MB/s]\n",
      "Downloading:  85%|████████▌ | 408M/478M [00:09<00:01, 44.5MB/s]\n",
      "Downloading:  88%|████████▊ | 422M/478M [00:09<00:01, 45.2MB/s]\n",
      "Downloading:  86%|████████▌ | 412M/478M [00:09<00:01, 44.9MB/s]\n",
      "Downloading:  89%|████████▉ | 427M/478M [00:09<00:01, 45.1MB/s]\n",
      "Downloading:  87%|████████▋ | 416M/478M [00:10<00:01, 45.3MB/s]\n",
      "Downloading:  90%|█████████ | 431M/478M [00:10<00:01, 45.2MB/s]\n",
      "Downloading:  88%|████████▊ | 421M/478M [00:10<00:01, 45.5MB/s]\n",
      "Downloading:  89%|████████▉ | 425M/478M [00:10<00:01, 45.7MB/s]\n",
      "Downloading:  91%|█████████ | 435M/478M [00:10<00:01, 28.5MB/s]\n",
      "Downloading:  92%|█████████▏| 439M/478M [00:10<00:01, 32.0MB/s]\n",
      "Downloading:  90%|████████▉ | 430M/478M [00:10<00:01, 33.2MB/s]\n",
      "Downloading:  93%|█████████▎| 444M/478M [00:10<00:01, 35.0MB/s]\n",
      "Downloading:  94%|█████████▍| 448M/478M [00:10<00:00, 37.7MB/s]\n",
      "Downloading:  91%|█████████ | 433M/478M [00:10<00:01, 30.9MB/s]\n",
      "Downloading:  95%|█████████▍| 452M/478M [00:10<00:00, 39.7MB/s]\n",
      "Downloading:  92%|█████████▏| 438M/478M [00:10<00:01, 34.3MB/s]\n",
      "Downloading:  96%|█████████▌| 457M/478M [00:10<00:00, 41.3MB/s]\n",
      "Downloading:  92%|█████████▏| 442M/478M [00:10<00:01, 37.1MB/s]\n",
      "Downloading:  96%|█████████▋| 461M/478M [00:10<00:00, 41.6MB/s]\n",
      "Downloading:  93%|█████████▎| 446M/478M [00:10<00:00, 39.3MB/s]\n",
      "Downloading:  97%|█████████▋| 465M/478M [00:11<00:00, 41.8MB/s]\n",
      "Downloading:  94%|█████████▍| 451M/478M [00:11<00:00, 41.0MB/s]\n",
      "Downloading:  98%|█████████▊| 469M/478M [00:11<00:00, 42.8MB/s]\n",
      "Downloading:  96%|█████████▌| 459M/478M [00:11<00:00, 43.5MB/s]\n",
      "Downloading:  99%|█████████▉| 474M/478M [00:11<00:00, 43.1MB/s]\n",
      "Downloading:  97%|█████████▋| 464M/478M [00:11<00:00, 44.2MB/s]\n",
      "Downloading: 100%|██████████| 478M/478M [00:11<00:00, 44.0MB/s]\n",
      "Downloading:  98%|█████████▊| 468M/478M [00:11<00:00, 44.8MB/s]\n",
      "Downloading:  99%|█████████▉| 473M/478M [00:11<00:00, 45.1MB/s]\n",
      "Downloading: 100%|█████████▉| 477M/478M [00:11<00:00, 45.8MB/s]\n",
      "Downloading: 100%|██████████| 478M/478M [00:11<00:00, 42.9MB/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset:  17%|█▋        | 1/6 [00:00<00:01,  4.95ba/s]\n",
      "Running tokenizer on dataset:  33%|███▎      | 2/6 [00:00<00:00,  5.36ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/6 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset:  50%|█████     | 3/6 [00:00<00:00,  5.20ba/s]\n",
      "Running tokenizer on dataset:  17%|█▋        | 1/6 [00:00<00:01,  4.87ba/s]\n",
      "Running tokenizer on dataset:  33%|███▎      | 2/6 [00:00<00:00,  5.26ba/s]\n",
      "Running tokenizer on dataset:  67%|██████▋   | 4/6 [00:00<00:00,  5.41ba/s]\n",
      "Running tokenizer on dataset:  50%|█████     | 3/6 [00:00<00:00,  5.42ba/s]\n",
      "Running tokenizer on dataset:  83%|████████▎ | 5/6 [00:00<00:00,  5.51ba/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 6/6 [00:01<00:00,  5.85ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset:  67%|██████▋   | 4/6 [00:00<00:00,  5.53ba/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 16.64ba/s]\n",
      "Running tokenizer on dataset:  83%|████████▎ | 5/6 [00:00<00:00,  5.60ba/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 6/6 [00:01<00:00,  5.94ba/s]\n",
      "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00, 16.06ba/s]\n",
      "Downloading builder script: 3.19kB [00:00, 1.54MB/s]                   \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]72.16.5.7)\u001b[0m \n",
      "Downloading builder script: 3.19kB [00:00, 1.92MB/s]                   \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]72.16.0.32)\u001b[0m \n",
      "  1%|          | 1/100 [00:00<00:28,  3.43it/s]7)\u001b[0m \n",
      "  2%|▏         | 2/100 [00:00<00:26,  3.68it/s]7)\u001b[0m \n",
      "  1%|          | 1/100 [00:00<00:27,  3.61it/s]32)\u001b[0m \n",
      "  3%|▎         | 3/100 [00:00<00:24,  3.89it/s]7)\u001b[0m \n",
      "  2%|▏         | 2/100 [00:00<00:25,  3.83it/s]32)\u001b[0m \n",
      "  4%|▍         | 4/100 [00:01<00:23,  4.07it/s]7)\u001b[0m \n",
      "  3%|▎         | 3/100 [00:00<00:24,  4.04it/s]32)\u001b[0m \n",
      "  5%|▌         | 5/100 [00:01<00:22,  4.17it/s]7)\u001b[0m \n",
      "  4%|▍         | 4/100 [00:00<00:22,  4.22it/s]32)\u001b[0m \n",
      "  6%|▌         | 6/100 [00:01<00:22,  4.25it/s]7)\u001b[0m \n",
      "  5%|▌         | 5/100 [00:01<00:22,  4.31it/s]32)\u001b[0m \n",
      "  6%|▌         | 6/100 [00:01<00:21,  4.37it/s]32)\u001b[0m \n",
      "  7%|▋         | 7/100 [00:01<00:21,  4.28it/s]7)\u001b[0m \n",
      "  8%|▊         | 8/100 [00:01<00:21,  4.31it/s]7)\u001b[0m \n",
      "  7%|▋         | 7/100 [00:01<00:21,  4.41it/s]32)\u001b[0m \n",
      "  9%|▉         | 9/100 [00:02<00:20,  4.35it/s]7)\u001b[0m \n",
      "  8%|▊         | 8/100 [00:01<00:20,  4.43it/s]32)\u001b[0m \n",
      " 10%|█         | 10/100 [00:02<00:20,  4.37it/s])\u001b[0m \n",
      "  9%|▉         | 9/100 [00:02<00:20,  4.46it/s]32)\u001b[0m \n",
      " 10%|█         | 10/100 [00:02<00:19,  4.50it/s]2)\u001b[0m \n",
      " 11%|█         | 11/100 [00:02<00:20,  4.42it/s])\u001b[0m \n",
      " 11%|█         | 11/100 [00:02<00:19,  4.54it/s]2)\u001b[0m \n",
      " 12%|█▏        | 12/100 [00:02<00:19,  4.45it/s])\u001b[0m \n",
      " 12%|█▏        | 12/100 [00:02<00:19,  4.56it/s]2)\u001b[0m \n",
      " 13%|█▎        | 13/100 [00:03<00:19,  4.46it/s])\u001b[0m \n",
      " 13%|█▎        | 13/100 [00:02<00:19,  4.57it/s]2)\u001b[0m \n",
      " 14%|█▍        | 14/100 [00:03<00:19,  4.48it/s])\u001b[0m \n",
      " 15%|█▌        | 15/100 [00:03<00:18,  4.48it/s])\u001b[0m \n",
      " 14%|█▍        | 14/100 [00:03<00:18,  4.57it/s]2)\u001b[0m \n",
      " 15%|█▌        | 15/100 [00:03<00:18,  4.59it/s]2)\u001b[0m \n",
      " 16%|█▌        | 16/100 [00:03<00:18,  4.48it/s])\u001b[0m \n",
      " 16%|█▌        | 16/100 [00:03<00:18,  4.59it/s]2)\u001b[0m \n",
      " 17%|█▋        | 17/100 [00:03<00:18,  4.49it/s])\u001b[0m \n",
      " 17%|█▋        | 17/100 [00:03<00:18,  4.60it/s]2)\u001b[0m \n",
      " 18%|█▊        | 18/100 [00:04<00:18,  4.51it/s])\u001b[0m \n",
      " 18%|█▊        | 18/100 [00:04<00:17,  4.60it/s]2)\u001b[0m \n",
      " 19%|█▉        | 19/100 [00:04<00:17,  4.51it/s])\u001b[0m \n",
      " 19%|█▉        | 19/100 [00:04<00:17,  4.60it/s]2)\u001b[0m \n",
      " 20%|██        | 20/100 [00:04<00:17,  4.51it/s])\u001b[0m \n",
      " 20%|██        | 20/100 [00:04<00:17,  4.61it/s]2)\u001b[0m \n",
      " 21%|██        | 21/100 [00:04<00:17,  4.53it/s])\u001b[0m \n",
      " 21%|██        | 21/100 [00:04<00:17,  4.61it/s]2)\u001b[0m \n",
      " 22%|██▏       | 22/100 [00:05<00:17,  4.54it/s])\u001b[0m \n",
      " 22%|██▏       | 22/100 [00:04<00:16,  4.61it/s]2)\u001b[0m \n",
      " 23%|██▎       | 23/100 [00:05<00:16,  4.55it/s])\u001b[0m \n",
      " 23%|██▎       | 23/100 [00:05<00:16,  4.61it/s]2)\u001b[0m \n",
      " 24%|██▍       | 24/100 [00:05<00:16,  4.54it/s])\u001b[0m \n",
      " 24%|██▍       | 24/100 [00:05<00:16,  4.61it/s]2)\u001b[0m \n",
      " 25%|██▌       | 25/100 [00:05<00:16,  4.50it/s])\u001b[0m \n",
      " 25%|██▌       | 25/100 [00:05<00:16,  4.61it/s]2)\u001b[0m \n",
      " 26%|██▌       | 26/100 [00:05<00:16,  4.51it/s])\u001b[0m \n",
      " 26%|██▌       | 26/100 [00:05<00:16,  4.60it/s]2)\u001b[0m \n",
      " 27%|██▋       | 27/100 [00:06<00:16,  4.53it/s])\u001b[0m \n",
      " 27%|██▋       | 27/100 [00:06<00:15,  4.61it/s]2)\u001b[0m \n",
      " 28%|██▊       | 28/100 [00:06<00:15,  4.54it/s])\u001b[0m \n",
      " 28%|██▊       | 28/100 [00:06<00:15,  4.61it/s]2)\u001b[0m \n",
      " 29%|██▉       | 29/100 [00:06<00:15,  4.52it/s])\u001b[0m \n",
      " 29%|██▉       | 29/100 [00:06<00:15,  4.61it/s]2)\u001b[0m \n",
      " 30%|███       | 30/100 [00:06<00:15,  4.53it/s])\u001b[0m \n",
      " 30%|███       | 30/100 [00:06<00:15,  4.61it/s]2)\u001b[0m \n",
      " 31%|███       | 31/100 [00:07<00:15,  4.53it/s])\u001b[0m \n",
      " 31%|███       | 31/100 [00:06<00:14,  4.61it/s]2)\u001b[0m \n",
      " 32%|███▏      | 32/100 [00:07<00:15,  4.52it/s])\u001b[0m \n",
      " 32%|███▏      | 32/100 [00:07<00:14,  4.60it/s]2)\u001b[0m \n",
      " 33%|███▎      | 33/100 [00:07<00:14,  4.53it/s])\u001b[0m \n",
      " 33%|███▎      | 33/100 [00:07<00:14,  4.60it/s]2)\u001b[0m \n",
      " 34%|███▍      | 34/100 [00:07<00:14,  4.51it/s])\u001b[0m \n",
      " 34%|███▍      | 34/100 [00:07<00:14,  4.60it/s]2)\u001b[0m \n",
      " 35%|███▌      | 35/100 [00:07<00:14,  4.49it/s])\u001b[0m \n",
      " 35%|███▌      | 35/100 [00:07<00:14,  4.61it/s]2)\u001b[0m \n",
      " 36%|███▌      | 36/100 [00:08<00:14,  4.50it/s])\u001b[0m \n",
      " 36%|███▌      | 36/100 [00:07<00:13,  4.60it/s]2)\u001b[0m \n",
      " 37%|███▋      | 37/100 [00:08<00:13,  4.53it/s])\u001b[0m \n",
      " 37%|███▋      | 37/100 [00:08<00:13,  4.61it/s]2)\u001b[0m \n",
      " 38%|███▊      | 38/100 [00:08<00:13,  4.54it/s])\u001b[0m \n",
      " 38%|███▊      | 38/100 [00:08<00:13,  4.60it/s]2)\u001b[0m \n",
      " 39%|███▉      | 39/100 [00:08<00:13,  4.53it/s])\u001b[0m \n",
      " 39%|███▉      | 39/100 [00:08<00:13,  4.59it/s]2)\u001b[0m \n",
      " 40%|████      | 40/100 [00:09<00:13,  4.54it/s])\u001b[0m \n",
      " 40%|████      | 40/100 [00:08<00:13,  4.60it/s]2)\u001b[0m \n",
      " 41%|████      | 41/100 [00:09<00:13,  4.54it/s])\u001b[0m \n",
      " 41%|████      | 41/100 [00:09<00:12,  4.60it/s]2)\u001b[0m \n",
      " 42%|████▏     | 42/100 [00:09<00:12,  4.53it/s])\u001b[0m \n",
      " 42%|████▏     | 42/100 [00:09<00:12,  4.60it/s]2)\u001b[0m \n",
      " 43%|████▎     | 43/100 [00:09<00:12,  4.53it/s])\u001b[0m \n",
      " 43%|████▎     | 43/100 [00:09<00:12,  4.60it/s]2)\u001b[0m \n",
      " 44%|████▍     | 44/100 [00:09<00:12,  4.52it/s])\u001b[0m \n",
      " 44%|████▍     | 44/100 [00:09<00:12,  4.59it/s]2)\u001b[0m \n",
      " 45%|████▌     | 45/100 [00:10<00:12,  4.51it/s])\u001b[0m \n",
      " 45%|████▌     | 45/100 [00:09<00:12,  4.58it/s]2)\u001b[0m \n",
      " 46%|████▌     | 46/100 [00:10<00:11,  4.54it/s])\u001b[0m \n",
      " 46%|████▌     | 46/100 [00:10<00:11,  4.59it/s]2)\u001b[0m \n",
      " 47%|████▋     | 47/100 [00:10<00:11,  4.54it/s])\u001b[0m \n",
      " 47%|████▋     | 47/100 [00:10<00:11,  4.59it/s]2)\u001b[0m \n",
      " 48%|████▊     | 48/100 [00:10<00:11,  4.53it/s])\u001b[0m \n",
      " 48%|████▊     | 48/100 [00:10<00:11,  4.60it/s]2)\u001b[0m \n",
      " 49%|████▉     | 49/100 [00:11<00:11,  4.53it/s])\u001b[0m \n",
      " 49%|████▉     | 49/100 [00:10<00:11,  4.60it/s]2)\u001b[0m \n",
      " 50%|█████     | 50/100 [00:11<00:11,  4.51it/s])\u001b[0m \n",
      " 50%|█████     | 50/100 [00:11<00:10,  4.60it/s]2)\u001b[0m \n",
      " 51%|█████     | 51/100 [00:11<00:10,  4.49it/s])\u001b[0m \n",
      " 51%|█████     | 51/100 [00:11<00:10,  4.60it/s]2)\u001b[0m \n",
      " 52%|█████▏    | 52/100 [00:11<00:10,  4.50it/s])\u001b[0m \n",
      " 52%|█████▏    | 52/100 [00:11<00:10,  4.60it/s]2)\u001b[0m \n",
      " 53%|█████▎    | 53/100 [00:11<00:10,  4.52it/s])\u001b[0m \n",
      " 53%|█████▎    | 53/100 [00:11<00:10,  4.60it/s]2)\u001b[0m \n",
      " 54%|█████▍    | 54/100 [00:12<00:10,  4.50it/s])\u001b[0m \n",
      " 54%|█████▍    | 54/100 [00:11<00:09,  4.60it/s]2)\u001b[0m \n",
      " 55%|█████▌    | 55/100 [00:12<00:09,  4.52it/s])\u001b[0m \n",
      " 55%|█████▌    | 55/100 [00:12<00:09,  4.60it/s]2)\u001b[0m \n",
      " 56%|█████▌    | 56/100 [00:12<00:09,  4.60it/s]2)\u001b[0m \n",
      " 56%|█████▌    | 56/100 [00:12<00:09,  4.53it/s])\u001b[0m \n",
      " 57%|█████▋    | 57/100 [00:12<00:09,  4.60it/s]2)\u001b[0m \n",
      " 57%|█████▋    | 57/100 [00:12<00:09,  4.54it/s])\u001b[0m \n",
      " 58%|█████▊    | 58/100 [00:12<00:09,  4.54it/s])\u001b[0m \n",
      " 58%|█████▊    | 58/100 [00:12<00:09,  4.60it/s]2)\u001b[0m \n",
      " 59%|█████▉    | 59/100 [00:13<00:09,  4.50it/s])\u001b[0m \n",
      " 59%|█████▉    | 59/100 [00:12<00:08,  4.61it/s]2)\u001b[0m \n",
      " 60%|██████    | 60/100 [00:13<00:08,  4.48it/s])\u001b[0m \n",
      " 60%|██████    | 60/100 [00:13<00:08,  4.61it/s]2)\u001b[0m \n",
      " 61%|██████    | 61/100 [00:13<00:08,  4.50it/s])\u001b[0m \n",
      " 61%|██████    | 61/100 [00:13<00:08,  4.61it/s]2)\u001b[0m \n",
      " 62%|██████▏   | 62/100 [00:13<00:08,  4.59it/s]2)\u001b[0m \n",
      " 62%|██████▏   | 62/100 [00:13<00:08,  4.51it/s])\u001b[0m \n",
      " 63%|██████▎   | 63/100 [00:13<00:08,  4.59it/s]2)\u001b[0m \n",
      " 63%|██████▎   | 63/100 [00:14<00:08,  4.51it/s])\u001b[0m \n",
      " 64%|██████▍   | 64/100 [00:14<00:07,  4.59it/s]2)\u001b[0m \n",
      " 64%|██████▍   | 64/100 [00:14<00:07,  4.51it/s])\u001b[0m \n",
      " 65%|██████▌   | 65/100 [00:14<00:07,  4.53it/s])\u001b[0m \n",
      " 65%|██████▌   | 65/100 [00:14<00:07,  4.60it/s]2)\u001b[0m \n",
      " 66%|██████▌   | 66/100 [00:14<00:07,  4.53it/s])\u001b[0m \n",
      " 66%|██████▌   | 66/100 [00:14<00:07,  4.60it/s]2)\u001b[0m \n",
      " 67%|██████▋   | 67/100 [00:14<00:07,  4.60it/s]2)\u001b[0m \n",
      " 67%|██████▋   | 67/100 [00:14<00:07,  4.52it/s])\u001b[0m \n",
      " 68%|██████▊   | 68/100 [00:14<00:06,  4.59it/s]2)\u001b[0m \n",
      " 68%|██████▊   | 68/100 [00:15<00:07,  4.52it/s])\u001b[0m \n",
      " 69%|██████▉   | 69/100 [00:15<00:06,  4.59it/s]2)\u001b[0m \n",
      " 69%|██████▉   | 69/100 [00:15<00:06,  4.49it/s])\u001b[0m \n",
      " 70%|███████   | 70/100 [00:15<00:06,  4.60it/s]2)\u001b[0m \n",
      " 70%|███████   | 70/100 [00:15<00:06,  4.49it/s])\u001b[0m \n",
      " 71%|███████   | 71/100 [00:15<00:06,  4.60it/s]2)\u001b[0m \n",
      " 71%|███████   | 71/100 [00:15<00:06,  4.52it/s])\u001b[0m \n",
      " 72%|███████▏  | 72/100 [00:16<00:06,  4.53it/s])\u001b[0m \n",
      " 72%|███████▏  | 72/100 [00:15<00:06,  4.60it/s]2)\u001b[0m \n",
      " 73%|███████▎  | 73/100 [00:16<00:05,  4.60it/s]2)\u001b[0m \n",
      " 73%|███████▎  | 73/100 [00:16<00:05,  4.52it/s])\u001b[0m \n",
      " 74%|███████▍  | 74/100 [00:16<00:05,  4.60it/s]2)\u001b[0m \n",
      " 74%|███████▍  | 74/100 [00:16<00:05,  4.52it/s])\u001b[0m \n",
      " 75%|███████▌  | 75/100 [00:16<00:05,  4.59it/s]2)\u001b[0m \n",
      " 75%|███████▌  | 75/100 [00:16<00:05,  4.52it/s])\u001b[0m \n",
      " 76%|███████▌  | 76/100 [00:16<00:05,  4.59it/s]2)\u001b[0m \n",
      " 76%|███████▌  | 76/100 [00:16<00:05,  4.51it/s])\u001b[0m \n",
      " 77%|███████▋  | 77/100 [00:16<00:05,  4.59it/s]2)\u001b[0m \n",
      " 77%|███████▋  | 77/100 [00:17<00:05,  4.52it/s])\u001b[0m \n",
      " 78%|███████▊  | 78/100 [00:17<00:04,  4.60it/s]2)\u001b[0m \n",
      " 78%|███████▊  | 78/100 [00:17<00:04,  4.50it/s])\u001b[0m \n",
      " 79%|███████▉  | 79/100 [00:17<00:04,  4.60it/s]2)\u001b[0m \n",
      " 79%|███████▉  | 79/100 [00:17<00:04,  4.49it/s])\u001b[0m \n",
      " 80%|████████  | 80/100 [00:17<00:04,  4.59it/s]2)\u001b[0m \n",
      " 80%|████████  | 80/100 [00:17<00:04,  4.51it/s])\u001b[0m \n",
      " 81%|████████  | 81/100 [00:17<00:04,  4.60it/s]2)\u001b[0m \n",
      " 81%|████████  | 81/100 [00:18<00:04,  4.53it/s])\u001b[0m \n",
      " 82%|████████▏ | 82/100 [00:17<00:03,  4.60it/s]2)\u001b[0m \n",
      " 82%|████████▏ | 82/100 [00:18<00:03,  4.52it/s])\u001b[0m \n",
      " 83%|████████▎ | 83/100 [00:18<00:03,  4.60it/s]2)\u001b[0m \n",
      " 83%|████████▎ | 83/100 [00:18<00:03,  4.51it/s])\u001b[0m \n",
      " 84%|████████▍ | 84/100 [00:18<00:03,  4.60it/s]2)\u001b[0m \n",
      " 84%|████████▍ | 84/100 [00:18<00:03,  4.50it/s])\u001b[0m \n",
      " 85%|████████▌ | 85/100 [00:18<00:03,  4.60it/s]2)\u001b[0m \n",
      " 85%|████████▌ | 85/100 [00:18<00:03,  4.48it/s])\u001b[0m \n",
      " 86%|████████▌ | 86/100 [00:18<00:03,  4.59it/s]2)\u001b[0m \n",
      " 86%|████████▌ | 86/100 [00:19<00:03,  4.49it/s])\u001b[0m \n",
      " 87%|████████▋ | 87/100 [00:19<00:02,  4.60it/s]2)\u001b[0m \n",
      " 87%|████████▋ | 87/100 [00:19<00:02,  4.49it/s])\u001b[0m \n",
      " 88%|████████▊ | 88/100 [00:19<00:02,  4.60it/s]2)\u001b[0m \n",
      " 88%|████████▊ | 88/100 [00:19<00:02,  4.48it/s])\u001b[0m \n",
      " 89%|████████▉ | 89/100 [00:19<00:02,  4.60it/s]2)\u001b[0m \n",
      " 89%|████████▉ | 89/100 [00:19<00:02,  4.51it/s])\u001b[0m \n",
      " 90%|█████████ | 90/100 [00:19<00:02,  4.60it/s]2)\u001b[0m \n",
      " 90%|█████████ | 90/100 [00:20<00:02,  4.52it/s])\u001b[0m \n",
      " 91%|█████████ | 91/100 [00:19<00:01,  4.60it/s]2)\u001b[0m \n",
      " 91%|█████████ | 91/100 [00:20<00:01,  4.52it/s])\u001b[0m \n",
      " 92%|█████████▏| 92/100 [00:20<00:01,  4.59it/s]2)\u001b[0m \n",
      " 92%|█████████▏| 92/100 [00:20<00:01,  4.51it/s])\u001b[0m \n",
      " 93%|█████████▎| 93/100 [00:20<00:01,  4.59it/s]2)\u001b[0m \n",
      " 93%|█████████▎| 93/100 [00:20<00:01,  4.50it/s])\u001b[0m \n",
      " 94%|█████████▍| 94/100 [00:20<00:01,  4.59it/s]2)\u001b[0m \n",
      " 94%|█████████▍| 94/100 [00:20<00:01,  4.48it/s])\u001b[0m \n",
      " 95%|█████████▌| 95/100 [00:20<00:01,  4.59it/s]2)\u001b[0m \n",
      " 95%|█████████▌| 95/100 [00:21<00:01,  4.49it/s])\u001b[0m \n",
      " 96%|█████████▌| 96/100 [00:21<00:00,  4.60it/s]2)\u001b[0m \n",
      " 96%|█████████▌| 96/100 [00:21<00:00,  4.51it/s])\u001b[0m \n",
      " 97%|█████████▋| 97/100 [00:21<00:00,  4.60it/s]2)\u001b[0m \n",
      " 97%|█████████▋| 97/100 [00:21<00:00,  4.48it/s])\u001b[0m \n",
      " 98%|█████████▊| 98/100 [00:21<00:00,  4.59it/s]2)\u001b[0m \n",
      " 98%|█████████▊| 98/100 [00:21<00:00,  4.49it/s])\u001b[0m \n",
      " 99%|█████████▉| 99/100 [00:21<00:00,  4.58it/s]2)\u001b[0m \n",
      " 99%|█████████▉| 99/100 [00:22<00:00,  4.52it/s])\u001b[0m \n",
      "100%|██████████| 100/100 [00:21<00:00,  4.59it/s])\u001b[0m \n",
      "100%|██████████| 100/100 [00:22<00:00,  4.53it/s]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=745, ip=172.16.0.32)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:24<00:00,  4.13it/s])\u001b[0m \n",
      "100%|██████████| 100/100 [00:24<00:00,  4.05it/s]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m To disable this warning, you can either:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m \t- Avoid using `tokenizers` before the fork if possible\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=747, ip=172.16.5.7)\u001b[0m \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#\"\"\" Finetuning a 🤗 Transformers model for sequence classification.\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from typing import Dict, Any\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5001')\n",
    "    \n",
    "import datasets\n",
    "import ray\n",
    "import transformers\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset, load_metric\n",
    "from ray.train import Trainer\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Finetune a transformers model on a text classification task\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Ignore this!\",\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--train_file\",\n",
    "        type=str,\n",
    "        default=\"data/train/part-algo-1-womens_clothing_ecommerce_reviews.csv\",\n",
    "        help=\"A csv or a json file containing the training data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_file\",\n",
    "        type=str,\n",
    "        default=\"data/validation/part-algo-1-womens_clothing_ecommerce_reviews.csv\",\n",
    "        help=\"A csv or a json file containing the validation data.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        help=(\n",
    "            \"The maximum total input sequence length after tokenization. \"\n",
    "            \"Sequences longer than this will be truncated, sequences shorter \"\n",
    "            \"will be padded if `--pad_to_max_lengh` is passed.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pad_to_max_length\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic \"\n",
    "        \"padding is used.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        help=\"Path to pretrained model or model identifier from \"\n",
    "        \"huggingface.co/models.\",\n",
    "        default=\"roberta-base\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_slow_tokenizer\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, will use a slow tokenizer (not backed by the 🤗 \"\n",
    "        \"Tokenizers library).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_train_batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Batch size (per device) for the training dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_eval_batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Batch size (per device) for the evaluation dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-5,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--weight_decay\", type=float, default=0.0, help=\"Weight decay to use.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Total number of training epochs to perform.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"Total number of training steps to perform. If provided, \"\n",
    "        \"overrides num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a \"\n",
    "        \"backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler_type\",\n",
    "        type=SchedulerType,\n",
    "        default=\"linear\",\n",
    "        help=\"The scheduler type to use.\",\n",
    "        choices=[\n",
    "            \"linear\",\n",
    "            \"cosine\",\n",
    "            \"cosine_with_restarts\",\n",
    "            \"polynomial\",\n",
    "            \"constant\",\n",
    "            \"constant_with_warmup\",\n",
    "        ],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_warmup_steps\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Number of steps for the warmup in the lr scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", type=str, default=None, help=\"Where to store the final model.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=None, help=\"A seed for reproducible training.\"\n",
    "    )\n",
    "\n",
    "    # Ray arguments.\n",
    "    parser.add_argument(\n",
    "        \"--start_local\", action=\"store_true\", help=\"Starts Ray on local machine.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--address\", \n",
    "        type=str, \n",
    "        default=\"127.0.0.1:6379\", \n",
    "        help=\"Ray address to connect to.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_workers\", \n",
    "        type=int, \n",
    "        default=2, \n",
    "        help=\"Number of workers to use.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_gpu\", action=\"store_true\", help=\"If training should be done on GPUs.\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Sanity checks\n",
    "    if (\n",
    "#        args.task_name is None\n",
    "        args.train_file is None\n",
    "        and args.validation_file is None\n",
    "    ):\n",
    "        raise ValueError(\"Need a training/validation file.\")\n",
    "    else:\n",
    "        if args.train_file is not None:\n",
    "            extension = args.train_file.split(\".\")[-1]\n",
    "            assert extension in [\n",
    "                \"csv\",\n",
    "                \"json\",\n",
    "            ], \"`train_file` should be a csv or a json file.\"\n",
    "        if args.validation_file is not None:\n",
    "            extension = args.validation_file.split(\".\")[-1]\n",
    "            assert extension in [\n",
    "                \"csv\",\n",
    "                \"json\",\n",
    "            ], \"`validation_file` should be a csv or a json file.\"\n",
    "\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_func(config: Dict[str, Any]):\n",
    "    args = config[\"args\"]\n",
    "    # Initialize the accelerator. We will let the accelerator handle device\n",
    "    # placement for us in this example.\n",
    "    accelerator = Accelerator()\n",
    "    # Make one log on every process with the configuration for debugging.\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.ERROR,\n",
    "    )\n",
    "    logger.info(accelerator.state)\n",
    "\n",
    "    # Setup logging, we only want one process per machine to log things on\n",
    "    # the screen. accelerator.is_local_main_process is only True for one\n",
    "    # process per machine.\n",
    "    logger.setLevel(\n",
    "        logging.ERROR if accelerator.is_local_main_process else logging.ERROR\n",
    "    )\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # If passed along, set the training seed now.\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON training and\n",
    "    # evaluation files (see below) or specify a GLUE benchmark task (the\n",
    "    # dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "    # For CSV/JSON files, this script will use as labels the column called\n",
    "    # 'label' and as pair of sentences the sentences in columns called\n",
    "    # 'sentence1' and 'sentence2' if such column exists or the first two\n",
    "    # columns not named label if at least two columns are provided.\n",
    "\n",
    "    # If the CSVs/JSONs contain only one non-label column, the script does\n",
    "    # single sentence classification on this single column. You can easily\n",
    "    # tweak this behavior (see below)\n",
    "\n",
    "    # In distributed training, the load_dataset function guarantee that only\n",
    "    # one local process can concurrently download the dataset.\n",
    "#    if args.task_name is not None:\n",
    "#        # Downloading and loading a dataset from the hub.\n",
    "#        raw_datasets = load_dataset(\"glue\", args.task_name)\n",
    "#    else:\n",
    "        # Loading the dataset from local csv or json file.\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    extension = (\n",
    "        args.train_file if args.train_file is not None else args.valid_file\n",
    "    ).split(\".\")[-1]\n",
    "\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "\n",
    "    label_list = raw_datasets[\"train\"].unique(\"sentiment\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # In distributed training, the .from_pretrained methods guarantee that\n",
    "    # only one local process can concurrently download model & vocab.\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        args.model_name_or_path, num_labels=num_labels, \n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path, use_fast=not args.use_slow_tokenizer\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Preprocessing the datasets\n",
    "    sentence1_key, sentence2_key = \"review_body\", None\n",
    "\n",
    "    # Some models have set the order of the labels to use,\n",
    "    # so let's make sure we do use it.\n",
    "    label_to_id = None\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "    if label_to_id is not None:\n",
    "        model.config.label2id = label_to_id\n",
    "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "    padding = \"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        texts = (\n",
    "            (examples[sentence1_key],)\n",
    "            if sentence2_key is None\n",
    "            else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(\n",
    "            *texts, padding=padding, max_length=args.max_length, truncation=True\n",
    "        )\n",
    "\n",
    "        if \"sentiment\" in examples:\n",
    "            if label_to_id is not None:\n",
    "                # Map labels to IDs (not necessary for GLUE tasks)\n",
    "                result[\"labels\"] = [\n",
    "                    label_to_id[l] for l in examples[\"sentiment\"]  # noqa:E741\n",
    "                ]\n",
    "            else:\n",
    "                # In all cases, rename the column to labels because the model\n",
    "                # will expect that.\n",
    "                result[\"labels\"] = examples[\"sentiment\"]\n",
    "\n",
    "        return result\n",
    "\n",
    "    processed_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"validation\"]\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    # DataLoaders creation:\n",
    "    if args.pad_to_max_length:\n",
    "        # If padding was already done ot max length, we use the default data\n",
    "        # collator that will just convert everything to tensors.\n",
    "        data_collator = default_data_collator\n",
    "    else:\n",
    "        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for\n",
    "        # us (by padding to the maximum length of the samples passed). When\n",
    "        # using mixed precision, we add `pad_to_multiple_of=8` to pad all\n",
    "        # tensors to multiple of 8s, which will enable the use of Tensor\n",
    "        # Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "            tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "        )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=args.per_device_train_batch_size,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=args.per_device_eval_batch_size,\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    # Split weights in two groups, one with weight decay and the other not.\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "#    model, optimizer, train_dataloader = accelerator.prepare(\n",
    "#        model, optimizer, train_dataloader\n",
    "#    )\n",
    "    # Note -> the training dataloader needs to be prepared before we grab\n",
    "    # his length below (cause its length will be shorter in multiprocess)\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / args.gradient_accumulation_steps\n",
    "    )\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    else:\n",
    "        args.num_train_epochs = math.ceil(\n",
    "            args.max_train_steps / num_update_steps_per_epoch\n",
    "        )\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=args.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.num_warmup_steps,\n",
    "        num_training_steps=args.max_train_steps,\n",
    "    )\n",
    "\n",
    "    # Get the metric function\n",
    "#    if args.task_name is not None:\n",
    "#        metric = load_metric(\"glue\", args.task_name)\n",
    "#    else:\n",
    "    metric = load_metric(\"accuracy\")\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = (\n",
    "        args.per_device_train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * args.gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num epochs = {args.num_train_epochs}\")\n",
    "    logger.info(\n",
    "        f\"  Instantaneous batch size per device =\"\n",
    "        f\" {args.per_device_train_batch_size}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"  Total train batch size (w. parallel, distributed & accumulation) \"\n",
    "        f\"= {total_batch_size}\"\n",
    "    )\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(\n",
    "        range(args.max_train_steps), disable=not accelerator.is_local_main_process\n",
    "    )\n",
    "    completed_steps = 0\n",
    "\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if (\n",
    "                step % args.gradient_accumulation_steps == 0\n",
    "                or step == len(train_dataloader) - 1\n",
    "            ):\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            predictions = (\n",
    "                outputs.logits.argmax(dim=-1)\n",
    "#                if not is_regression\n",
    "#                else outputs.logits.squeeze()\n",
    "            )\n",
    "            metric.add_batch(\n",
    "                predictions=accelerator.gather(predictions),\n",
    "                references=accelerator.gather(batch[\"labels\"]),\n",
    "            )\n",
    "\n",
    "        mlflow.log_param(\"batch_size\", args.per_device_train_batch_size)\n",
    "        mlflow.log_param(\"learning_rate\", args.learning_rate)\n",
    "        mlflow.log_param(\"weight_decay\", args.weight_decay)\n",
    "        mlflow.log_param(\"max_length\", args.max_length)\n",
    "\n",
    "        eval_metric = metric.compute()\n",
    "        mlflow.log_metric(\"accuracy\", eval_metric['accuracy'])\n",
    "        \n",
    "        logger.info(f\"epoch {epoch}: {eval_metric}\")\n",
    "\n",
    "    if args.output_dir is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    config = {\"args\": args}\n",
    "    args.use_gpu = True\n",
    "\n",
    "    if args.start_local or args.address or args.num_workers > 1 or args.use_gpu:\n",
    "        if args.start_local:\n",
    "            # Start a local Ray runtime.\n",
    "            ray.init(num_cpus=args.num_workers)\n",
    "        else:\n",
    "            # Connect to a Ray cluster for distributed training.\n",
    "            ray.init(address=args.address)\n",
    "        trainer = Trainer(\"torch\", num_workers=args.num_workers, use_gpu=args.use_gpu,\n",
    "                          resources_per_worker={'CPU': 4, 'GPU': 1})\n",
    "        trainer.start()\n",
    "        trainer.run(train_func, config)\n",
    "    else:\n",
    "        # Run training locally.\n",
    "        train_func(config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b409397-9ae1-43f9-a238-eab3c6c2141e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
